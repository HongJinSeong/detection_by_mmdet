{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4a90950-bd80-41eb-8f20-fec74578bd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmdet.apis import init_detector, inference_detector, show_result_pyplot\n",
    "import mmcv\n",
    "from mmcv import Config\n",
    "\n",
    "\n",
    "import copy\n",
    "import os.path as osp\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from mmdet.datasets.builder import DATASETS\n",
    "from mmdet.datasets.custom import CustomDataset\n",
    "\n",
    "from mmdet.apis import set_random_seed\n",
    "\n",
    "from mmdet.core.post_processing import bbox_nms\n",
    "\n",
    "\n",
    "import base64\n",
    "\n",
    "import glob as _glob\n",
    "import os\n",
    "\n",
    "import json\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import pandas as pd \n",
    "\n",
    "from ensemble_boxes import *\n",
    "\n",
    "from mmcv.visualization import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d99a0f96-ed65-438c-a29a-77bfbfd9dea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def glob(dir, pats, recursive=False):  # faster than match, python3 only\n",
    "    pats = pats if isinstance(pats, (list, tuple)) else [pats]\n",
    "    matches = []\n",
    "    for pat in pats:\n",
    "        matches += _glob.glob(os.path.join(dir, pat), recursive=recursive)\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30e8ed22-5cc9-42bb-bcc4-c61a73b83d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 추가수정 기존 받았던 pretrain과 매칭되는 config로 수정 \n",
    "cfg = Config.fromfile('mmdetection/configs/resnest/cascade_rcnn_s101_fpn_syncbn-backbone+head_mstrain-range_1x_coco.py') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61768152-f124-4cf5-b1fb-a1cf7a236052",
   "metadata": {},
   "outputs": [],
   "source": [
    "@DATASETS.register_module()\n",
    "class lesion_ds(CustomDataset):\n",
    "    CLASSES=('01_ulcer','02_mass','04_lymph', '05_bleeding')\n",
    "\n",
    "\n",
    "    def load_annotations(self, ann_file):\n",
    "        \n",
    "        CLASSES_dict = { '01_ulcer' : 0 , '02_mass' : 1, '04_lymph' : 2, '05_bleeding' : 3}\n",
    "        \n",
    "        # load image list from file\n",
    "        image_list = mmcv.list_from_file(self.ann_file)\n",
    "        \n",
    "        data_infos = []\n",
    "        \n",
    "        for idx,img in enumerate(image_list):\n",
    "            json_data = {}\n",
    "            with open(img, \"r\") as json_file:\n",
    "                json_data = json.load(json_file)\n",
    "            \n",
    "            filename = img # json에 annotation + image라서 json 자체를 filename로 주고 LoadImageFromFile을 baseline을 참조하여 custom으로 \n",
    "\n",
    "            height = json_data['imageHeight']\n",
    "            width = json_data['imageWidth']\n",
    "\n",
    "            data_info = dict(filename=filename, width=width, height=height)\n",
    "\n",
    "            gt_bboxes = []\n",
    "            gt_labels = []\n",
    "\n",
    "            for a_idx in range(len(json_data['shapes'])):\n",
    "                gt_labels.append(CLASSES_dict[json_data['shapes'][a_idx]['label']])\n",
    "                \n",
    "                ## 좌표순서 좌상 우상 우하 좌하 \n",
    "                ## mmdetection의 default annotation loader는 'coco_panoptic.py'에 있는데 여기 기준으로 하면 bbox에는 x1,y1,x2,y2가 담겨야함\n",
    "                ## 만약에 이렇게 따로 load를 하지 않을시에는 x,y,w,h 형태로 annotaion을 생성하면 자동으로 x1,y1,x2,y2로 변환해줌 \n",
    "                ori_pos = np.array(json_data['shapes'][a_idx]['points'])\n",
    "                x1, y1, x2, y2 = min(ori_pos[:, 0]), min(ori_pos[:, 1]), max(ori_pos[:, 0]), max(ori_pos[:, 1])\n",
    "                \n",
    "                if x1 == 0 : \n",
    "                    x1 = 1 \n",
    "                if y1 == 0 : \n",
    "                    y1 = 1 \n",
    "                \n",
    "                if x2 == width: \n",
    "                    x2 = x2 - 1 \n",
    "                if y2 == width: \n",
    "                    y2 = y2-1 \n",
    "            \n",
    "                \n",
    "                if x1==x2 or y1==y2:\n",
    "                    print('Grond-truth Bounding Box 이상체크')\n",
    "                    print(filename)\n",
    "                     \n",
    "                \n",
    "                gt_bboxes.append([x1,y1,x2,y2])\n",
    "                \n",
    "\n",
    "            data_anno = dict(\n",
    "                    bboxes=np.array(gt_bboxes, dtype=np.float32).reshape(-1, 4),\n",
    "                    labels=np.array(gt_labels, dtype=np.long))\n",
    "\n",
    "\n",
    "            data_info.update(ann=data_anno)\n",
    "            data_infos.append(data_info)\n",
    "            \n",
    "            if idx!=0 and idx%20000==0:\n",
    "                print(str(idx)+'/'+str(len(image_list))+' load annotations END!')\n",
    "            \n",
    "        \n",
    "        \n",
    "        return data_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ce22367-2ea3-4d74-9702-6c7678d8581d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "model = dict(\n",
      "    type='CascadeRCNN',\n",
      "    backbone=dict(\n",
      "        type='ResNeSt',\n",
      "        depth=101,\n",
      "        num_stages=4,\n",
      "        out_indices=(0, 1, 2, 3),\n",
      "        frozen_stages=1,\n",
      "        norm_cfg=dict(type='BN', requires_grad=True),\n",
      "        norm_eval=False,\n",
      "        style='pytorch',\n",
      "        init_cfg=dict(type='Pretrained', checkpoint='open-mmlab://resnest101'),\n",
      "        stem_channels=128,\n",
      "        radix=2,\n",
      "        reduction_factor=4,\n",
      "        avg_down_stride=True),\n",
      "    neck=dict(\n",
      "        type='FPN',\n",
      "        in_channels=[256, 512, 1024, 2048],\n",
      "        out_channels=256,\n",
      "        num_outs=5),\n",
      "    rpn_head=dict(\n",
      "        type='RPNHead',\n",
      "        in_channels=256,\n",
      "        feat_channels=256,\n",
      "        anchor_generator=dict(\n",
      "            type='AnchorGenerator',\n",
      "            scales=[8],\n",
      "            ratios=[0.5, 1.0, 2.0],\n",
      "            strides=[4, 8, 16, 32, 64]),\n",
      "        bbox_coder=dict(\n",
      "            type='DeltaXYWHBBoxCoder',\n",
      "            target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "            target_stds=[1.0, 1.0, 1.0, 1.0]),\n",
      "        loss_cls=dict(\n",
      "            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n",
      "        loss_bbox=dict(\n",
      "            type='SmoothL1Loss', beta=0.1111111111111111, loss_weight=1.0)),\n",
      "    roi_head=dict(\n",
      "        type='CascadeRoIHead',\n",
      "        num_stages=3,\n",
      "        stage_loss_weights=[1, 0.5, 0.25],\n",
      "        bbox_roi_extractor=dict(\n",
      "            type='SingleRoIExtractor',\n",
      "            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),\n",
      "            out_channels=256,\n",
      "            featmap_strides=[4, 8, 16, 32]),\n",
      "        bbox_head=[\n",
      "            dict(\n",
      "                type='Shared4Conv1FCBBoxHead',\n",
      "                in_channels=256,\n",
      "                conv_out_channels=256,\n",
      "                fc_out_channels=1024,\n",
      "                norm_cfg=dict(type='BN', requires_grad=True),\n",
      "                roi_feat_size=7,\n",
      "                num_classes=4,\n",
      "                bbox_coder=dict(\n",
      "                    type='DeltaXYWHBBoxCoder',\n",
      "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "                    target_stds=[0.1, 0.1, 0.2, 0.2]),\n",
      "                reg_class_agnostic=True,\n",
      "                loss_cls=dict(\n",
      "                    type='CrossEntropyLoss',\n",
      "                    use_sigmoid=False,\n",
      "                    loss_weight=1.0),\n",
      "                loss_bbox=dict(type='SmoothL1Loss', beta=1.0,\n",
      "                               loss_weight=1.0)),\n",
      "            dict(\n",
      "                type='Shared4Conv1FCBBoxHead',\n",
      "                in_channels=256,\n",
      "                conv_out_channels=256,\n",
      "                fc_out_channels=1024,\n",
      "                norm_cfg=dict(type='BN', requires_grad=True),\n",
      "                roi_feat_size=7,\n",
      "                num_classes=4,\n",
      "                bbox_coder=dict(\n",
      "                    type='DeltaXYWHBBoxCoder',\n",
      "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "                    target_stds=[0.05, 0.05, 0.1, 0.1]),\n",
      "                reg_class_agnostic=True,\n",
      "                loss_cls=dict(\n",
      "                    type='CrossEntropyLoss',\n",
      "                    use_sigmoid=False,\n",
      "                    loss_weight=1.0),\n",
      "                loss_bbox=dict(type='SmoothL1Loss', beta=1.0,\n",
      "                               loss_weight=1.0)),\n",
      "            dict(\n",
      "                type='Shared4Conv1FCBBoxHead',\n",
      "                in_channels=256,\n",
      "                conv_out_channels=256,\n",
      "                fc_out_channels=1024,\n",
      "                norm_cfg=dict(type='BN', requires_grad=True),\n",
      "                roi_feat_size=7,\n",
      "                num_classes=4,\n",
      "                bbox_coder=dict(\n",
      "                    type='DeltaXYWHBBoxCoder',\n",
      "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "                    target_stds=[0.033, 0.033, 0.067, 0.067]),\n",
      "                reg_class_agnostic=True,\n",
      "                loss_cls=dict(\n",
      "                    type='CrossEntropyLoss',\n",
      "                    use_sigmoid=False,\n",
      "                    loss_weight=1.0),\n",
      "                loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0))\n",
      "        ]),\n",
      "    train_cfg=dict(\n",
      "        rpn=dict(\n",
      "            assigner=dict(\n",
      "                type='MaxIoUAssigner',\n",
      "                pos_iou_thr=0.7,\n",
      "                neg_iou_thr=0.3,\n",
      "                min_pos_iou=0.3,\n",
      "                match_low_quality=True,\n",
      "                ignore_iof_thr=-1),\n",
      "            sampler=dict(\n",
      "                type='RandomSampler',\n",
      "                num=256,\n",
      "                pos_fraction=0.5,\n",
      "                neg_pos_ub=-1,\n",
      "                add_gt_as_proposals=False),\n",
      "            allowed_border=0,\n",
      "            pos_weight=-1,\n",
      "            debug=False),\n",
      "        rpn_proposal=dict(\n",
      "            nms_pre=2000,\n",
      "            max_per_img=2000,\n",
      "            nms=dict(type='nms', iou_threshold=0.7),\n",
      "            min_bbox_size=0),\n",
      "        rcnn=[\n",
      "            dict(\n",
      "                assigner=dict(\n",
      "                    type='MaxIoUAssigner',\n",
      "                    pos_iou_thr=0.5,\n",
      "                    neg_iou_thr=0.5,\n",
      "                    min_pos_iou=0.5,\n",
      "                    match_low_quality=False,\n",
      "                    ignore_iof_thr=-1),\n",
      "                sampler=dict(\n",
      "                    type='RandomSampler',\n",
      "                    num=512,\n",
      "                    pos_fraction=0.25,\n",
      "                    neg_pos_ub=-1,\n",
      "                    add_gt_as_proposals=True),\n",
      "                pos_weight=-1,\n",
      "                debug=False),\n",
      "            dict(\n",
      "                assigner=dict(\n",
      "                    type='MaxIoUAssigner',\n",
      "                    pos_iou_thr=0.6,\n",
      "                    neg_iou_thr=0.6,\n",
      "                    min_pos_iou=0.6,\n",
      "                    match_low_quality=False,\n",
      "                    ignore_iof_thr=-1),\n",
      "                sampler=dict(\n",
      "                    type='RandomSampler',\n",
      "                    num=512,\n",
      "                    pos_fraction=0.25,\n",
      "                    neg_pos_ub=-1,\n",
      "                    add_gt_as_proposals=True),\n",
      "                pos_weight=-1,\n",
      "                debug=False),\n",
      "            dict(\n",
      "                assigner=dict(\n",
      "                    type='MaxIoUAssigner',\n",
      "                    pos_iou_thr=0.7,\n",
      "                    neg_iou_thr=0.7,\n",
      "                    min_pos_iou=0.7,\n",
      "                    match_low_quality=False,\n",
      "                    ignore_iof_thr=-1),\n",
      "                sampler=dict(\n",
      "                    type='RandomSampler',\n",
      "                    num=512,\n",
      "                    pos_fraction=0.25,\n",
      "                    neg_pos_ub=-1,\n",
      "                    add_gt_as_proposals=True),\n",
      "                pos_weight=-1,\n",
      "                debug=False)\n",
      "        ]),\n",
      "    test_cfg=dict(\n",
      "        rpn=dict(\n",
      "            nms_pre=1000,\n",
      "            max_per_img=1000,\n",
      "            nms=dict(type='nms', iou_threshold=0.7),\n",
      "            min_bbox_size=0),\n",
      "        rcnn=dict(\n",
      "            score_thr=0.05,\n",
      "            nms=dict(type='nms', iou_threshold=0.5),\n",
      "            max_per_img=100)))\n",
      "dataset_type = 'CocoDataset'\n",
      "data_root = 'data/coco/'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n",
      "train_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(\n",
      "        type='LoadAnnotations',\n",
      "        with_bbox=True,\n",
      "        with_mask=False,\n",
      "        poly2mask=False),\n",
      "    dict(\n",
      "        type='Resize',\n",
      "        img_scale=[(1333, 640), (1333, 800)],\n",
      "        multiscale_mode='range',\n",
      "        keep_ratio=True),\n",
      "    dict(type='RandomFlip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.68, 116.779, 103.939],\n",
      "        std=[58.393, 57.12, 57.375],\n",
      "        to_rgb=True),\n",
      "    dict(type='Pad', size_divisor=32),\n",
      "    dict(type='DefaultFormatBundle'),\n",
      "    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(type='CUSTOM_LoadImageFromFile'),\n",
      "    dict(\n",
      "        type='MultiScaleFlipAug',\n",
      "        img_scale=(600, 600),\n",
      "        flip=False,\n",
      "        transforms=[\n",
      "            dict(type='Resize', keep_ratio=True),\n",
      "            dict(type='RandomFlip'),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_rgb=True),\n",
      "            dict(type='Pad', size_divisor=32),\n",
      "            dict(type='ImageToTensor', keys=['img']),\n",
      "            dict(type='Collect', keys=['img'])\n",
      "        ])\n",
      "]\n",
      "data = dict(\n",
      "    test=dict(\n",
      "        pipeline=[\n",
      "            dict(type='CUSTOM_LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(600, 600),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[123.675, 116.28, 103.53],\n",
      "                        std=[58.395, 57.12, 57.375],\n",
      "                        to_rgb=True),\n",
      "                    dict(type='Pad', size_divisor=32),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='Collect', keys=['img'])\n",
      "                ])\n",
      "        ],\n",
      "        type='CocoDataset'))\n",
      "evaluation = dict(interval=1, metric='bbox')\n",
      "optimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=None)\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    warmup='linear',\n",
      "    warmup_iters=500,\n",
      "    warmup_ratio=0.001,\n",
      "    step=[8, 11])\n",
      "runner = dict(type='EpochBasedRunner', max_epochs=12)\n",
      "checkpoint_config = dict(interval=1)\n",
      "log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])\n",
      "custom_hooks = [dict(type='NumClassCheckHook')]\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = None\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "norm_cfg = dict(type='BN', requires_grad=True)\n",
      "seed = 0\n",
      "gpu_ids = [0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sync bn하면 single gpu라 error 발생 \n",
    "cfg.norm_cfg = dict(type='BN', requires_grad=True)\n",
    "\n",
    "#class 4개로 \n",
    "cfg.model.roi_head.bbox_head[0].num_classes=4\n",
    "\n",
    "cfg.model.roi_head.bbox_head[1].num_classes=4\n",
    "\n",
    "cfg.model.roi_head.bbox_head[2].num_classes=4\n",
    "\n",
    "#데이터 pipeline은 config 참조 \n",
    "cfg.img_norm_cfg = dict(\n",
    "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n",
    "\n",
    "## 실제 test시에는 Test Time Augmentation 적용하는 것으로 \n",
    "cfg.test_pipeline = [\n",
    "    dict(type='CUSTOM_LoadImageFromFile'),\n",
    "    dict(\n",
    "        type='MultiScaleFlipAug',\n",
    "        img_scale=(600, 600),\n",
    "        flip=False,\n",
    "        transforms=[\n",
    "            dict(type='Resize', keep_ratio=True),\n",
    "            dict(type='RandomFlip'),\n",
    "            dict(type='Normalize', **cfg.img_norm_cfg),\n",
    "            dict(type='Pad', size_divisor=32),\n",
    "            dict(type='ImageToTensor', keys=['img']),\n",
    "            dict(type='Collect', keys=['img'])\n",
    "        ])\n",
    "]\n",
    "cfg.data = dict(\n",
    "    test=dict(pipeline=cfg.test_pipeline))\n",
    "\n",
    "\n",
    "cfg.data.test.type = cfg.dataset_type\n",
    "\n",
    "\n",
    "# seed set\n",
    "cfg.seed = 0\n",
    "set_random_seed(0, deterministic=False)\n",
    "cfg.gpu_ids = [0]\n",
    "\n",
    "print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1a4179d-fd1c-4e0f-8cd8-0d258d58070f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from local path: lesion_checkpoints_ver2/epoch_10.pth\n",
      "Config (path: mmdetection/configs/resnest/cascade_rcnn_s101_fpn_syncbn-backbone+head_mstrain-range_1x_coco.py): {'model': {'type': 'CascadeRCNN', 'backbone': {'type': 'ResNeSt', 'depth': 101, 'num_stages': 4, 'out_indices': (0, 1, 2, 3), 'frozen_stages': 1, 'norm_cfg': {'type': 'BN', 'requires_grad': True}, 'norm_eval': False, 'style': 'pytorch', 'init_cfg': {'type': 'Pretrained', 'checkpoint': 'open-mmlab://resnest101'}, 'stem_channels': 128, 'radix': 2, 'reduction_factor': 4, 'avg_down_stride': True}, 'neck': {'type': 'FPN', 'in_channels': [256, 512, 1024, 2048], 'out_channels': 256, 'num_outs': 5}, 'rpn_head': {'type': 'RPNHead', 'in_channels': 256, 'feat_channels': 256, 'anchor_generator': {'type': 'AnchorGenerator', 'scales': [8], 'ratios': [0.5, 1.0, 2.0], 'strides': [4, 8, 16, 32, 64]}, 'bbox_coder': {'type': 'DeltaXYWHBBoxCoder', 'target_means': [0.0, 0.0, 0.0, 0.0], 'target_stds': [1.0, 1.0, 1.0, 1.0]}, 'loss_cls': {'type': 'CrossEntropyLoss', 'use_sigmoid': True, 'loss_weight': 1.0}, 'loss_bbox': {'type': 'SmoothL1Loss', 'beta': 0.1111111111111111, 'loss_weight': 1.0}}, 'roi_head': {'type': 'CascadeRoIHead', 'num_stages': 3, 'stage_loss_weights': [1, 0.5, 0.25], 'bbox_roi_extractor': {'type': 'SingleRoIExtractor', 'roi_layer': {'type': 'RoIAlign', 'output_size': 7, 'sampling_ratio': 0}, 'out_channels': 256, 'featmap_strides': [4, 8, 16, 32]}, 'bbox_head': [{'type': 'Shared4Conv1FCBBoxHead', 'in_channels': 256, 'conv_out_channels': 256, 'fc_out_channels': 1024, 'norm_cfg': {'type': 'BN', 'requires_grad': True}, 'roi_feat_size': 7, 'num_classes': 4, 'bbox_coder': {'type': 'DeltaXYWHBBoxCoder', 'target_means': [0.0, 0.0, 0.0, 0.0], 'target_stds': [0.1, 0.1, 0.2, 0.2]}, 'reg_class_agnostic': True, 'loss_cls': {'type': 'CrossEntropyLoss', 'use_sigmoid': False, 'loss_weight': 1.0}, 'loss_bbox': {'type': 'SmoothL1Loss', 'beta': 1.0, 'loss_weight': 1.0}}, {'type': 'Shared4Conv1FCBBoxHead', 'in_channels': 256, 'conv_out_channels': 256, 'fc_out_channels': 1024, 'norm_cfg': {'type': 'BN', 'requires_grad': True}, 'roi_feat_size': 7, 'num_classes': 4, 'bbox_coder': {'type': 'DeltaXYWHBBoxCoder', 'target_means': [0.0, 0.0, 0.0, 0.0], 'target_stds': [0.05, 0.05, 0.1, 0.1]}, 'reg_class_agnostic': True, 'loss_cls': {'type': 'CrossEntropyLoss', 'use_sigmoid': False, 'loss_weight': 1.0}, 'loss_bbox': {'type': 'SmoothL1Loss', 'beta': 1.0, 'loss_weight': 1.0}}, {'type': 'Shared4Conv1FCBBoxHead', 'in_channels': 256, 'conv_out_channels': 256, 'fc_out_channels': 1024, 'norm_cfg': {'type': 'BN', 'requires_grad': True}, 'roi_feat_size': 7, 'num_classes': 4, 'bbox_coder': {'type': 'DeltaXYWHBBoxCoder', 'target_means': [0.0, 0.0, 0.0, 0.0], 'target_stds': [0.033, 0.033, 0.067, 0.067]}, 'reg_class_agnostic': True, 'loss_cls': {'type': 'CrossEntropyLoss', 'use_sigmoid': False, 'loss_weight': 1.0}, 'loss_bbox': {'type': 'SmoothL1Loss', 'beta': 1.0, 'loss_weight': 1.0}}], 'train_cfg': None, 'test_cfg': {'score_thr': 0.05, 'nms': {'type': 'nms', 'iou_threshold': 0.5}, 'max_per_img': 100}, 'pretrained': None}, 'train_cfg': None, 'test_cfg': {'rpn': {'nms_pre': 1000, 'max_per_img': 1000, 'nms': {'type': 'nms', 'iou_threshold': 0.7}, 'min_bbox_size': 0}, 'rcnn': {'score_thr': 0.05, 'nms': {'type': 'nms', 'iou_threshold': 0.5}, 'max_per_img': 100}}, 'pretrained': None}, 'dataset_type': 'CocoDataset', 'data_root': 'data/coco/', 'img_norm_cfg': {'mean': [123.675, 116.28, 103.53], 'std': [58.395, 57.12, 57.375], 'to_rgb': True}, 'train_pipeline': [{'type': 'LoadImageFromFile'}, {'type': 'LoadAnnotations', 'with_bbox': True, 'with_mask': False, 'poly2mask': False}, {'type': 'Resize', 'img_scale': [(1333, 640), (1333, 800)], 'multiscale_mode': 'range', 'keep_ratio': True}, {'type': 'RandomFlip', 'flip_ratio': 0.5}, {'type': 'Normalize', 'mean': [123.68, 116.779, 103.939], 'std': [58.393, 57.12, 57.375], 'to_rgb': True}, {'type': 'Pad', 'size_divisor': 32}, {'type': 'DefaultFormatBundle'}, {'type': 'Collect', 'keys': ['img', 'gt_bboxes', 'gt_labels']}], 'test_pipeline': [{'type': 'CUSTOM_LoadImageFromFile'}, {'type': 'MultiScaleFlipAug', 'img_scale': (600, 600), 'flip': False, 'transforms': [{'type': 'Resize', 'keep_ratio': True}, {'type': 'RandomFlip'}, {'type': 'Normalize', 'mean': [123.675, 116.28, 103.53], 'std': [58.395, 57.12, 57.375], 'to_rgb': True}, {'type': 'Pad', 'size_divisor': 32}, {'type': 'ImageToTensor', 'keys': ['img']}, {'type': 'Collect', 'keys': ['img']}]}], 'data': {'test': {'pipeline': [{'type': 'CUSTOM_LoadImageFromFile'}, {'type': 'MultiScaleFlipAug', 'img_scale': (600, 600), 'flip': False, 'transforms': [{'type': 'Resize', 'keep_ratio': True}, {'type': 'RandomFlip'}, {'type': 'Normalize', 'mean': [123.675, 116.28, 103.53], 'std': [58.395, 57.12, 57.375], 'to_rgb': True}, {'type': 'Pad', 'size_divisor': 32}, {'type': 'ImageToTensor', 'keys': ['img']}, {'type': 'Collect', 'keys': ['img']}]}], 'type': 'CocoDataset'}}, 'evaluation': {'interval': 1, 'metric': 'bbox'}, 'optimizer': {'type': 'SGD', 'lr': 0.02, 'momentum': 0.9, 'weight_decay': 0.0001}, 'optimizer_config': {'grad_clip': None}, 'lr_config': {'policy': 'step', 'warmup': 'linear', 'warmup_iters': 500, 'warmup_ratio': 0.001, 'step': [8, 11]}, 'runner': {'type': 'EpochBasedRunner', 'max_epochs': 12}, 'checkpoint_config': {'interval': 1}, 'log_config': {'interval': 50, 'hooks': [{'type': 'TextLoggerHook'}]}, 'custom_hooks': [{'type': 'NumClassCheckHook'}], 'dist_params': {'backend': 'nccl'}, 'log_level': 'INFO', 'load_from': None, 'resume_from': None, 'workflow': [('train', 1)], 'norm_cfg': {'type': 'BN', 'requires_grad': True}, 'seed': 0, 'gpu_ids': [0]}\n"
     ]
    }
   ],
   "source": [
    "# Build the detector\n",
    "#model = init_detector(config, checkpoint, device='cuda:0')\n",
    "checkpoint='lesion_checkpoints_ver2/epoch_10.pth'\n",
    "# model = init_detector(cfg,checkpoint, device='cpu')\n",
    "model = init_detector(cfg, checkpoint, device='cuda:0')\n",
    "model = model.eval()\n",
    "print(model.cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f2cfd45-85dc-4764-99d9-4e8217e274e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 추가수정 기존 받았던 pretrain과 매칭되는 config로 수정 \n",
    "cfg_defor = Config.fromfile('mmdetection/configs/deformable_detr/deformable_detr_twostage_refine_r50_16x2_50e_coco.py') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a024e583-e2fd-4137-bb5b-53429d4c5e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "dataset_type = 'lesion_ds'\n",
      "data_root = ''\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n",
      "train_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(type='LoadAnnotations', with_bbox=True),\n",
      "    dict(type='RandomFlip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='AutoAugment',\n",
      "        policies=[[{\n",
      "            'type':\n",
      "            'Resize',\n",
      "            'img_scale': [(480, 1333), (512, 1333), (544, 1333), (576, 1333),\n",
      "                          (608, 1333), (640, 1333), (672, 1333), (704, 1333),\n",
      "                          (736, 1333), (768, 1333), (800, 1333)],\n",
      "            'multiscale_mode':\n",
      "            'value',\n",
      "            'keep_ratio':\n",
      "            True\n",
      "        }],\n",
      "                  [{\n",
      "                      'type': 'Resize',\n",
      "                      'img_scale': [(400, 4200), (500, 4200), (600, 4200)],\n",
      "                      'multiscale_mode': 'value',\n",
      "                      'keep_ratio': True\n",
      "                  }, {\n",
      "                      'type': 'RandomCrop',\n",
      "                      'crop_type': 'absolute_range',\n",
      "                      'crop_size': (384, 600),\n",
      "                      'allow_negative_crop': True\n",
      "                  }, {\n",
      "                      'type':\n",
      "                      'Resize',\n",
      "                      'img_scale': [(480, 1333), (512, 1333), (544, 1333),\n",
      "                                    (576, 1333), (608, 1333), (640, 1333),\n",
      "                                    (672, 1333), (704, 1333), (736, 1333),\n",
      "                                    (768, 1333), (800, 1333)],\n",
      "                      'multiscale_mode':\n",
      "                      'value',\n",
      "                      'override':\n",
      "                      True,\n",
      "                      'keep_ratio':\n",
      "                      True\n",
      "                  }]]),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_rgb=True),\n",
      "    dict(type='Pad', size_divisor=1),\n",
      "    dict(type='DefaultFormatBundle'),\n",
      "    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(type='CUSTOM_LoadImageFromFile'),\n",
      "    dict(\n",
      "        type='MultiScaleFlipAug',\n",
      "        img_scale=(736, 736),\n",
      "        flip=False,\n",
      "        transforms=[\n",
      "            dict(type='Resize', keep_ratio=True),\n",
      "            dict(type='RandomFlip'),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_rgb=True),\n",
      "            dict(type='Pad', size_divisor=1),\n",
      "            dict(type='ImageToTensor', keys=['img']),\n",
      "            dict(type='Collect', keys=['img'])\n",
      "        ])\n",
      "]\n",
      "data = dict(\n",
      "    test=dict(\n",
      "        pipeline=[\n",
      "            dict(type='CUSTOM_LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(736, 736),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[123.675, 116.28, 103.53],\n",
      "                        std=[58.395, 57.12, 57.375],\n",
      "                        to_rgb=True),\n",
      "                    dict(type='Pad', size_divisor=1),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='Collect', keys=['img'])\n",
      "                ])\n",
      "        ],\n",
      "        type='lesion_ds'))\n",
      "evaluation = dict(interval=1, metric='mAP')\n",
      "checkpoint_config = dict(interval=1)\n",
      "log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])\n",
      "custom_hooks = [dict(type='NumClassCheckHook')]\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = None\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "model = dict(\n",
      "    type='DeformableDETR',\n",
      "    backbone=dict(\n",
      "        type='ResNet',\n",
      "        depth=50,\n",
      "        num_stages=4,\n",
      "        out_indices=(1, 2, 3),\n",
      "        frozen_stages=1,\n",
      "        norm_cfg=dict(type='BN', requires_grad=False),\n",
      "        norm_eval=True,\n",
      "        style='pytorch',\n",
      "        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')),\n",
      "    neck=dict(\n",
      "        type='ChannelMapper',\n",
      "        in_channels=[512, 1024, 2048],\n",
      "        kernel_size=1,\n",
      "        out_channels=256,\n",
      "        act_cfg=None,\n",
      "        norm_cfg=dict(type='GN', num_groups=4),\n",
      "        num_outs=4),\n",
      "    bbox_head=dict(\n",
      "        type='DeformableDETRHead',\n",
      "        num_query=300,\n",
      "        num_classes=4,\n",
      "        in_channels=2048,\n",
      "        sync_cls_avg_factor=True,\n",
      "        as_two_stage=True,\n",
      "        transformer=dict(\n",
      "            type='DeformableDetrTransformer',\n",
      "            encoder=dict(\n",
      "                type='DetrTransformerEncoder',\n",
      "                num_layers=6,\n",
      "                transformerlayers=dict(\n",
      "                    type='BaseTransformerLayer',\n",
      "                    attn_cfgs=dict(\n",
      "                        type='MultiScaleDeformableAttention', embed_dims=256),\n",
      "                    feedforward_channels=1024,\n",
      "                    ffn_dropout=0.1,\n",
      "                    operation_order=('self_attn', 'norm', 'ffn', 'norm'))),\n",
      "            decoder=dict(\n",
      "                type='DeformableDetrTransformerDecoder',\n",
      "                num_layers=6,\n",
      "                return_intermediate=True,\n",
      "                transformerlayers=dict(\n",
      "                    type='DetrTransformerDecoderLayer',\n",
      "                    attn_cfgs=[\n",
      "                        dict(\n",
      "                            type='MultiheadAttention',\n",
      "                            embed_dims=256,\n",
      "                            num_heads=8,\n",
      "                            dropout=0.1),\n",
      "                        dict(\n",
      "                            type='MultiScaleDeformableAttention',\n",
      "                            embed_dims=256)\n",
      "                    ],\n",
      "                    feedforward_channels=1024,\n",
      "                    ffn_dropout=0.1,\n",
      "                    operation_order=('self_attn', 'norm', 'cross_attn', 'norm',\n",
      "                                     'ffn', 'norm')))),\n",
      "        positional_encoding=dict(\n",
      "            type='SinePositionalEncoding',\n",
      "            num_feats=128,\n",
      "            normalize=True,\n",
      "            offset=-0.5),\n",
      "        loss_cls=dict(\n",
      "            type='FocalLoss',\n",
      "            use_sigmoid=True,\n",
      "            gamma=2.0,\n",
      "            alpha=0.25,\n",
      "            loss_weight=2.0),\n",
      "        loss_bbox=dict(type='L1Loss', loss_weight=5.0),\n",
      "        loss_iou=dict(type='GIoULoss', loss_weight=1.0),\n",
      "        with_box_refine=True),\n",
      "    train_cfg=dict(\n",
      "        assigner=dict(\n",
      "            type='HungarianAssigner',\n",
      "            cls_cost=dict(type='FocalLossCost', weight=2.0),\n",
      "            reg_cost=dict(type='BBoxL1Cost', weight=5.0, box_format='xywh'),\n",
      "            iou_cost=dict(type='IoUCost', iou_mode='giou', weight=1.0))),\n",
      "    test_cfg=dict(max_per_img=100))\n",
      "optimizer = dict(\n",
      "    type='AdamW',\n",
      "    lr=0.0002,\n",
      "    weight_decay=0.0001,\n",
      "    paramwise_cfg=dict(\n",
      "        custom_keys=dict(\n",
      "            backbone=dict(lr_mult=0.1),\n",
      "            sampling_offsets=dict(lr_mult=0.1),\n",
      "            reference_points=dict(lr_mult=0.1))))\n",
      "optimizer_config = dict(grad_clip=dict(max_norm=0.1, norm_type=2))\n",
      "lr_config = dict(policy='step', step=[40])\n",
      "runner = dict(type='EpochBasedRunner', max_epochs=50)\n",
      "work_dir = 'lesion_checkpoints_ver1'\n",
      "seed = 0\n",
      "gpu_ids = [0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cfg_defor.dataset_type  = 'lesion_ds'\n",
    "cfg_defor.data_root = ''\n",
    "\n",
    "cfg_defor.work_dir = 'lesion_checkpoints_ver1'\n",
    "\n",
    "cfg_defor.evaluation.metric = 'mAP'\n",
    "\n",
    "#class 갯수 매칭 \n",
    "cfg_defor.model.bbox_head.num_classes=4\n",
    "\n",
    "#데이터 pipeline은 config 참조 \n",
    "cfg_defor.img_norm_cfg = dict(\n",
    "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n",
    "\n",
    "\n",
    "# test_pipeline, NOTE the Pad's size_divisor is different from the default\n",
    "# setting (size_divisor=32). While there is little effect on the performance\n",
    "# whether we use the default setting or use size_divisor=1.\n",
    "cfg_defor.test_pipeline = [\n",
    "    dict(type='CUSTOM_LoadImageFromFile'),\n",
    "    dict(\n",
    "        type='MultiScaleFlipAug',\n",
    "        img_scale=(736, 736),\n",
    "        flip=False,\n",
    "        transforms=[\n",
    "            dict(type='Resize', keep_ratio=True),\n",
    "            dict(type='RandomFlip'),\n",
    "            dict(type='Normalize', **cfg_defor.img_norm_cfg),\n",
    "            dict(type='Pad', size_divisor=1),\n",
    "            dict(type='ImageToTensor', keys=['img']),\n",
    "            dict(type='Collect', keys=['img'])\n",
    "        ])\n",
    "]\n",
    "\n",
    "# batch size 3060에 맞게 set (기존 pretrain시에는  batch 32 사용했다고 되어있음)\n",
    "cfg_defor.data = dict(\n",
    "    test=dict(pipeline=cfg_defor.test_pipeline))\n",
    "\n",
    "\n",
    "cfg_defor.data.test.type = cfg_defor.dataset_type\n",
    "\n",
    "cfg_defor.model.neck.norm_cfg = dict(type='GN', num_groups=4) #batch size 매칭 \n",
    "\n",
    "# seed set\n",
    "cfg_defor.seed = 0\n",
    "set_random_seed(0, deterministic=False)\n",
    "cfg_defor.gpu_ids = [0]\n",
    "\n",
    "print(f'Config:\\n{cfg_defor.pretty_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9fdb123-3101-4a4e-9be4-b380e5ec5104",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/mmcv/cnn/bricks/transformer.py:342: UserWarning: The arguments `feedforward_channels` in BaseTransformerLayer has been deprecated, now you should set `feedforward_channels` and other FFN related arguments to a dict named `ffn_cfgs`. \n",
      "  f'The arguments `{ori_name}` in BaseTransformerLayer '\n",
      "/opt/conda/lib/python3.7/site-packages/mmcv/cnn/bricks/transformer.py:342: UserWarning: The arguments `ffn_dropout` in BaseTransformerLayer has been deprecated, now you should set `ffn_drop` and other FFN related arguments to a dict named `ffn_cfgs`. \n",
      "  f'The arguments `{ori_name}` in BaseTransformerLayer '\n",
      "/opt/conda/lib/python3.7/site-packages/mmcv/cnn/bricks/transformer.py:342: UserWarning: The arguments `ffn_num_fcs` in BaseTransformerLayer has been deprecated, now you should set `num_fcs` and other FFN related arguments to a dict named `ffn_cfgs`. \n",
      "  f'The arguments `{ori_name}` in BaseTransformerLayer '\n",
      "/opt/conda/lib/python3.7/site-packages/mmcv/cnn/bricks/transformer.py:92: UserWarning: The arguments `dropout` in MultiheadAttention has been deprecated, now you can separately set `attn_drop`(float), proj_drop(float), and `dropout_layer`(dict) \n",
      "  warnings.warn('The arguments `dropout` in MultiheadAttention '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from local path: lesion_checkpoints_ver1/epoch_50.pth\n",
      "Config (path: mmdetection/configs/deformable_detr/deformable_detr_twostage_refine_r50_16x2_50e_coco.py): {'dataset_type': 'lesion_ds', 'data_root': '', 'img_norm_cfg': {'mean': [123.675, 116.28, 103.53], 'std': [58.395, 57.12, 57.375], 'to_rgb': True}, 'train_pipeline': [{'type': 'LoadImageFromFile'}, {'type': 'LoadAnnotations', 'with_bbox': True}, {'type': 'RandomFlip', 'flip_ratio': 0.5}, {'type': 'AutoAugment', 'policies': [[{'type': 'Resize', 'img_scale': [(480, 1333), (512, 1333), (544, 1333), (576, 1333), (608, 1333), (640, 1333), (672, 1333), (704, 1333), (736, 1333), (768, 1333), (800, 1333)], 'multiscale_mode': 'value', 'keep_ratio': True}], [{'type': 'Resize', 'img_scale': [(400, 4200), (500, 4200), (600, 4200)], 'multiscale_mode': 'value', 'keep_ratio': True}, {'type': 'RandomCrop', 'crop_type': 'absolute_range', 'crop_size': (384, 600), 'allow_negative_crop': True}, {'type': 'Resize', 'img_scale': [(480, 1333), (512, 1333), (544, 1333), (576, 1333), (608, 1333), (640, 1333), (672, 1333), (704, 1333), (736, 1333), (768, 1333), (800, 1333)], 'multiscale_mode': 'value', 'override': True, 'keep_ratio': True}]]}, {'type': 'Normalize', 'mean': [123.675, 116.28, 103.53], 'std': [58.395, 57.12, 57.375], 'to_rgb': True}, {'type': 'Pad', 'size_divisor': 1}, {'type': 'DefaultFormatBundle'}, {'type': 'Collect', 'keys': ['img', 'gt_bboxes', 'gt_labels']}], 'test_pipeline': [{'type': 'CUSTOM_LoadImageFromFile'}, {'type': 'MultiScaleFlipAug', 'img_scale': (736, 736), 'flip': False, 'transforms': [{'type': 'Resize', 'keep_ratio': True}, {'type': 'RandomFlip'}, {'type': 'Normalize', 'mean': [123.675, 116.28, 103.53], 'std': [58.395, 57.12, 57.375], 'to_rgb': True}, {'type': 'Pad', 'size_divisor': 1}, {'type': 'ImageToTensor', 'keys': ['img']}, {'type': 'Collect', 'keys': ['img']}]}], 'data': {'test': {'pipeline': [{'type': 'CUSTOM_LoadImageFromFile'}, {'type': 'MultiScaleFlipAug', 'img_scale': (736, 736), 'flip': False, 'transforms': [{'type': 'Resize', 'keep_ratio': True}, {'type': 'RandomFlip'}, {'type': 'Normalize', 'mean': [123.675, 116.28, 103.53], 'std': [58.395, 57.12, 57.375], 'to_rgb': True}, {'type': 'Pad', 'size_divisor': 1}, {'type': 'ImageToTensor', 'keys': ['img']}, {'type': 'Collect', 'keys': ['img']}]}], 'type': 'lesion_ds'}}, 'evaluation': {'interval': 1, 'metric': 'mAP'}, 'checkpoint_config': {'interval': 1}, 'log_config': {'interval': 50, 'hooks': [{'type': 'TextLoggerHook'}]}, 'custom_hooks': [{'type': 'NumClassCheckHook'}], 'dist_params': {'backend': 'nccl'}, 'log_level': 'INFO', 'load_from': None, 'resume_from': None, 'workflow': [('train', 1)], 'model': {'type': 'DeformableDETR', 'backbone': {'type': 'ResNet', 'depth': 50, 'num_stages': 4, 'out_indices': (1, 2, 3), 'frozen_stages': 1, 'norm_cfg': {'type': 'BN', 'requires_grad': False}, 'norm_eval': True, 'style': 'pytorch', 'init_cfg': {'type': 'Pretrained', 'checkpoint': 'torchvision://resnet50'}}, 'neck': {'type': 'ChannelMapper', 'in_channels': [512, 1024, 2048], 'kernel_size': 1, 'out_channels': 256, 'act_cfg': None, 'norm_cfg': {'type': 'GN', 'num_groups': 4}, 'num_outs': 4}, 'bbox_head': {'type': 'DeformableDETRHead', 'num_query': 300, 'num_classes': 4, 'in_channels': 2048, 'sync_cls_avg_factor': True, 'as_two_stage': True, 'transformer': {'type': 'DeformableDetrTransformer', 'encoder': {'type': 'DetrTransformerEncoder', 'num_layers': 6, 'transformerlayers': {'type': 'BaseTransformerLayer', 'attn_cfgs': {'type': 'MultiScaleDeformableAttention', 'embed_dims': 256}, 'feedforward_channels': 1024, 'ffn_dropout': 0.1, 'operation_order': ('self_attn', 'norm', 'ffn', 'norm')}}, 'decoder': {'type': 'DeformableDetrTransformerDecoder', 'num_layers': 6, 'return_intermediate': True, 'transformerlayers': {'type': 'DetrTransformerDecoderLayer', 'attn_cfgs': [{'type': 'MultiheadAttention', 'embed_dims': 256, 'num_heads': 8, 'dropout': 0.1}, {'type': 'MultiScaleDeformableAttention', 'embed_dims': 256}], 'feedforward_channels': 1024, 'ffn_dropout': 0.1, 'operation_order': ('self_attn', 'norm', 'cross_attn', 'norm', 'ffn', 'norm')}}, 'as_two_stage': True}, 'positional_encoding': {'type': 'SinePositionalEncoding', 'num_feats': 128, 'normalize': True, 'offset': -0.5}, 'loss_cls': {'type': 'FocalLoss', 'use_sigmoid': True, 'gamma': 2.0, 'alpha': 0.25, 'loss_weight': 2.0}, 'loss_bbox': {'type': 'L1Loss', 'loss_weight': 5.0}, 'loss_iou': {'type': 'GIoULoss', 'loss_weight': 1.0}, 'with_box_refine': True, 'train_cfg': None, 'test_cfg': {'max_per_img': 100}}, 'train_cfg': None, 'test_cfg': {'max_per_img': 100}, 'pretrained': None}, 'optimizer': {'type': 'AdamW', 'lr': 0.0002, 'weight_decay': 0.0001, 'paramwise_cfg': {'custom_keys': {'backbone': {'lr_mult': 0.1}, 'sampling_offsets': {'lr_mult': 0.1}, 'reference_points': {'lr_mult': 0.1}}}}, 'optimizer_config': {'grad_clip': {'max_norm': 0.1, 'norm_type': 2}}, 'lr_config': {'policy': 'step', 'step': [40]}, 'runner': {'type': 'EpochBasedRunner', 'max_epochs': 50}, 'work_dir': 'lesion_checkpoints_ver1', 'seed': 0, 'gpu_ids': [0]}\n"
     ]
    }
   ],
   "source": [
    "# Build the detector\n",
    "#model_defor = init_detector(cfg_defor,checkpoint, device='cpu')\n",
    "checkpoint='lesion_checkpoints_ver1/epoch_50.pth'\n",
    "model_defor = init_detector(cfg_defor,checkpoint, device='cuda:0')\n",
    "model_defor = model_defor.eval()\n",
    "print(model_defor.cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40868f3c-5274-4e00-a915-c73a11c50cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ls=glob('lesion_DB/test','*')\n",
    "test_ls.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41835ddb-deae-46bc-90fd-833b33629180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 end!\n",
      "100 end!\n",
      "200 end!\n",
      "300 end!\n",
      "400 end!\n",
      "500 end!\n",
      "600 end!\n",
      "700 end!\n",
      "800 end!\n",
      "900 end!\n",
      "1000 end!\n",
      "1100 end!\n",
      "1200 end!\n",
      "1300 end!\n",
      "1400 end!\n",
      "1500 end!\n",
      "1600 end!\n",
      "1700 end!\n",
      "1800 end!\n",
      "1900 end!\n",
      "2000 end!\n",
      "2100 end!\n",
      "2200 end!\n",
      "2300 end!\n",
      "2400 end!\n",
      "2500 end!\n",
      "2600 end!\n",
      "2700 end!\n",
      "2800 end!\n",
      "2900 end!\n",
      "3000 end!\n",
      "3100 end!\n",
      "3200 end!\n",
      "3300 end!\n",
      "3400 end!\n",
      "3500 end!\n",
      "3600 end!\n",
      "3700 end!\n",
      "3800 end!\n",
      "3900 end!\n",
      "4000 end!\n",
      "4100 end!\n",
      "4200 end!\n",
      "4300 end!\n",
      "4400 end!\n",
      "4500 end!\n",
      "4600 end!\n",
      "4700 end!\n",
      "4800 end!\n",
      "4900 end!\n",
      "5000 end!\n",
      "5100 end!\n",
      "5200 end!\n",
      "5300 end!\n",
      "5400 end!\n",
      "5500 end!\n",
      "5600 end!\n",
      "5700 end!\n",
      "5800 end!\n",
      "5900 end!\n",
      "6000 end!\n",
      "6100 end!\n",
      "6200 end!\n",
      "6300 end!\n",
      "6400 end!\n",
      "6500 end!\n",
      "6600 end!\n",
      "6700 end!\n",
      "6800 end!\n",
      "6900 end!\n",
      "7000 end!\n",
      "7100 end!\n",
      "7200 end!\n",
      "7300 end!\n",
      "7400 end!\n",
      "7500 end!\n",
      "7600 end!\n",
      "7700 end!\n",
      "7800 end!\n",
      "7900 end!\n",
      "8000 end!\n",
      "8100 end!\n",
      "8200 end!\n",
      "8300 end!\n",
      "8400 end!\n",
      "8500 end!\n",
      "8600 end!\n",
      "8700 end!\n",
      "8800 end!\n",
      "8900 end!\n",
      "9000 end!\n",
      "9100 end!\n",
      "9200 end!\n",
      "9300 end!\n",
      "9400 end!\n",
      "9500 end!\n",
      "9600 end!\n",
      "9700 end!\n",
      "9800 end!\n",
      "9900 end!\n",
      "10000 end!\n",
      "10100 end!\n",
      "10200 end!\n",
      "10300 end!\n",
      "10400 end!\n",
      "10500 end!\n",
      "10600 end!\n",
      "10700 end!\n",
      "10800 end!\n",
      "10900 end!\n",
      "11000 end!\n",
      "11100 end!\n",
      "11200 end!\n",
      "11300 end!\n",
      "11400 end!\n",
      "11500 end!\n",
      "11600 end!\n",
      "11700 end!\n",
      "11800 end!\n",
      "11900 end!\n",
      "12000 end!\n",
      "12100 end!\n",
      "12200 end!\n",
      "12300 end!\n",
      "12400 end!\n",
      "12500 end!\n",
      "12600 end!\n",
      "12700 end!\n",
      "12800 end!\n",
      "12900 end!\n",
      "13000 end!\n",
      "13100 end!\n",
      "13200 end!\n",
      "13300 end!\n",
      "13400 end!\n",
      "13500 end!\n",
      "13600 end!\n",
      "13700 end!\n",
      "13800 end!\n",
      "13900 end!\n",
      "14000 end!\n",
      "14100 end!\n",
      "14200 end!\n",
      "14300 end!\n",
      "14400 end!\n",
      "14500 end!\n",
      "14600 end!\n",
      "14700 end!\n",
      "14800 end!\n",
      "14900 end!\n",
      "15000 end!\n",
      "15100 end!\n",
      "15200 end!\n",
      "15300 end!\n",
      "15400 end!\n",
      "15500 end!\n",
      "15600 end!\n",
      "15700 end!\n",
      "15800 end!\n",
      "15900 end!\n",
      "16000 end!\n",
      "16100 end!\n",
      "16200 end!\n",
      "16300 end!\n",
      "16400 end!\n",
      "16500 end!\n",
      "16600 end!\n",
      "16700 end!\n",
      "16800 end!\n",
      "16900 end!\n",
      "17000 end!\n",
      "17100 end!\n",
      "17200 end!\n",
      "17300 end!\n",
      "17400 end!\n",
      "17500 end!\n",
      "17600 end!\n",
      "17700 end!\n",
      "17800 end!\n",
      "17900 end!\n",
      "18000 end!\n",
      "18100 end!\n",
      "18200 end!\n",
      "18300 end!\n",
      "18400 end!\n",
      "18500 end!\n",
      "18600 end!\n",
      "18700 end!\n",
      "18800 end!\n",
      "18900 end!\n",
      "19000 end!\n",
      "19100 end!\n",
      "19200 end!\n",
      "19300 end!\n",
      "19400 end!\n",
      "19500 end!\n",
      "19600 end!\n",
      "19700 end!\n",
      "19800 end!\n",
      "19900 end!\n",
      "20000 end!\n",
      "20100 end!\n",
      "20200 end!\n",
      "20300 end!\n",
      "20400 end!\n",
      "20500 end!\n",
      "20600 end!\n",
      "20700 end!\n",
      "20800 end!\n"
     ]
    }
   ],
   "source": [
    "CLASSES_dict = { 0 : '01_ulcer' , 1 : '02_mass' , 2 : '04_lymph' , 3 : '05_bleeding' }\n",
    "CLASS_NAME=['01_ulcer','02_mass','04_lymph','05_bleeding']\n",
    "\n",
    "results_for_csv = {\n",
    "    'file_name':[], 'class_id':[], 'confidence':[], 'point1_x':[], 'point1_y':[],\n",
    "    'point2_x':[], 'point2_y':[], 'point3_x':[], 'point3_y':[], 'point4_x':[], 'point4_y':[]\n",
    "}\n",
    "\n",
    "def IoU(box1, box2):\n",
    "    # box = (x1, y1, x2, y2)\n",
    "    box1_area = (box1[2] - box1[0] + 1) * (box1[3] - box1[1] + 1)\n",
    "    box2_area = (box2[2] - box2[0] + 1) * (box2[3] - box2[1] + 1)\n",
    "\n",
    "    # obtain x1, y1, x2, y2 of the intersection\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "\n",
    "    # compute the width and height of the intersection\n",
    "    w = max(0, x2 - x1 + 1)\n",
    "    h = max(0, y2 - y1 + 1)\n",
    "\n",
    "    inter = w * h\n",
    "    iou = inter / (box1_area + box2_area - inter)\n",
    "    return iou\n",
    "\n",
    "\n",
    "### weighted boxes fusion 사용 \n",
    "### https://github.com/ZFTurbo/Weighted-Boxes-Fusion 참조 \n",
    "\n",
    "for t_idx, filename in enumerate(test_ls):\n",
    "    boxes_list=[]\n",
    "    score_list=[]\n",
    "    labels_list=[]\n",
    "    weights = [2, 1] ## 첫번째 model이 이후의 모델보다 valdiation score가 높았음으로 더 높게 줌 \n",
    "\n",
    "    \n",
    "    with open(filename, \"r\") as json_file:\n",
    "            json_data = json.load(json_file)\n",
    "\n",
    "    img_bytes = base64.b64decode(json_data['imageData'])\n",
    "\n",
    "    img = mmcv.imfrombytes(img_bytes, flag='color',backend='pillow')\n",
    "\n",
    "    img = img.astype(np.float32)\n",
    "    \n",
    "    height, width = img.shape[:2]\n",
    "    \n",
    "    res1_box=[]\n",
    "    res1_sco=[]\n",
    "    res1_lbl=[]\n",
    "    \n",
    "    ## output shape x1, y1, x2, y2 , confidence score\n",
    "    results_1 = inference_detector(model, img)\n",
    "\n",
    "\n",
    "    for cls1,res1 in enumerate(results_1):\n",
    "        if res1.shape[0]!=0:\n",
    "            for r1 in res1:\n",
    "                ## 좌표 값 0~1로 정규화\n",
    "                r1[0] = r1[0] / width\n",
    "                r1[1] = r1[1] / height\n",
    "                r1[2] = r1[2] / width\n",
    "                r1[3] = r1[3] / height\n",
    "                res1_box.append(r1[0:4].tolist())\n",
    "                res1_sco.append(float(r1[4]))\n",
    "                res1_lbl.append(cls1)\n",
    "            \n",
    "    boxes_list.append(res1_box)\n",
    "    score_list.append(res1_sco)\n",
    "    labels_list.append(res1_lbl)\n",
    "    \n",
    "#     res2_box=[]\n",
    "#     res2_sco=[]\n",
    "#     res2_lbl=[]\n",
    "    \n",
    "#     results_2 = inference_detector(model_defor, img)\n",
    "\n",
    "    \n",
    "#     for cls2,res2 in enumerate(results_2):\n",
    "#         if res2.shape[0]!=0:\n",
    "#             for r2 in res2:\n",
    "#                 ## 좌표 값 0~1로 정규화\n",
    "#                 r2[0] = r2[0] / width\n",
    "#                 r2[1] = r2[1] / height\n",
    "#                 r2[2] = r2[2] / width\n",
    "#                 r2[3] = r2[3] / height\n",
    "#                 res2_box.append(r2[0:4].tolist())\n",
    "#                 res2_sco.append(float(r2[4]))\n",
    "#                 res2_lbl.append(cls2)\n",
    "                \n",
    "#     boxes_list.append(res2_box)\n",
    "#     score_list.append(res2_sco)\n",
    "#     labels_list.append(res2_lbl)\n",
    "    \n",
    "    ## 제출 갯수 제한이 있기때문에 skip_box_thr 값을 조정하여 적정한 output 갯수(30000개정도) 나오도록 유도 \n",
    "    iou_thr = 0.55\n",
    "    skip_box_thr = 0.2\n",
    "    sigma = 0.1\n",
    "    \n",
    "    # boxes, scores, labels = weighted_boxes_fusion(boxes_list, score_list, labels_list, weights=weights, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n",
    "    \n",
    "#     print(boxes)\n",
    "#     print(scores)\n",
    "#     print(labels)\n",
    "    \n",
    "#     iou_thr = 0.4\n",
    "#     skip_box_thr = 0.08\n",
    "    \n",
    "    boxes, scores, labels = weighted_boxes_fusion(boxes_list, score_list, labels_list, weights=None, iou_thr=iou_thr)\n",
    "    \n",
    "#     print(boxes)\n",
    "#     print(scores)\n",
    "#     print(labels)\n",
    "    \n",
    "    for f_idx in range(len(boxes)):\n",
    "        results_for_csv['file_name'].append(filename.split('/')[-1])\n",
    "        results_for_csv['class_id'].append(labels[f_idx]+1)\n",
    "        results_for_csv['confidence'].append(scores[f_idx])\n",
    "        results_for_csv['point1_x'].append(boxes[f_idx][0]*576)\n",
    "        results_for_csv['point1_y'].append(boxes[f_idx][1]*576)\n",
    "        results_for_csv['point2_x'].append(boxes[f_idx][2]*576)\n",
    "        results_for_csv['point2_y'].append(boxes[f_idx][1]*576)\n",
    "        results_for_csv['point3_x'].append(boxes[f_idx][2]*576)\n",
    "        results_for_csv['point3_y'].append(boxes[f_idx][3]*576)\n",
    "        results_for_csv['point4_x'].append(boxes[f_idx][0]*576)\n",
    "        results_for_csv['point4_y'].append(boxes[f_idx][3]*576)\n",
    "#         print(results_for_csv)\n",
    "    \n",
    "    \n",
    "#     break\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## 단순 threshold 값 기준으로 하면 겹치는 box가 너무많이 생겨서 발생하는 문제 해결이 안되기 때문에 NMS로 겹쳐지는 박스의 경우 제거 해주는 것이 필요해보임 \n",
    "    # for cls_idx, result in enumerate(results):\n",
    "    #     if result.shape[0]!=0:\n",
    "    #         out_nms = nms(result[:,0:4],result[:,4],0.4)\n",
    "    #         save_target = result[out_nms]\n",
    "    #         for pos_idx, pos, in enumerate(save_target):\n",
    "    #             results_for_csv['file_name'].append(filename.split('/')[-1])\n",
    "    #             results_for_csv['class_id'].append(cls_idx+1)\n",
    "    #             results_for_csv['confidence'].append(pos[4])\n",
    "    #             results_for_csv['point1_x'].append(pos[0])\n",
    "    #             results_for_csv['point1_y'].append(pos[1])\n",
    "    #             results_for_csv['point2_x'].append(pos[2])\n",
    "    #             results_for_csv['point2_y'].append(pos[1])\n",
    "    #             results_for_csv['point3_x'].append(pos[2])\n",
    "    #             results_for_csv['point3_y'].append(pos[3])\n",
    "    #             results_for_csv['point4_x'].append(pos[0])\n",
    "    #             results_for_csv['point4_y'].append(pos[3])\n",
    "    \n",
    "    \n",
    "    ## 적정 threshold 값 모르겠으니 일단 test output 싹다 저장해서 이미지 봐도 어느정도 score가 좋을지 잘모르겠음..\n",
    "    # model.show_result(img,results,score_thr=0.0,out_file='infer_show_checkpoints10/'+filename.split('/')[-1][:-5]+'.png')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ## result list로 0 1 2 3  ==> 4개 class \n",
    "    ## class id 는 1부터 시작하니까 +1 해줌 \n",
    "    #  0.407    0.89\n",
    "    # for cls_idx, result in enumerate(results):\n",
    "    #     ## output이 0이 아니고 \n",
    "    #     if result.shape[0]!=0:\n",
    "    #         # print(result)\n",
    "    #         thre_val = result[np.where(result[:,4]>=0.407)]\n",
    "    #         if thre_val.shape[0]!=0:\n",
    "    #             for pos_idx, pos, in enumerate(thre_val):\n",
    "    #                 results_for_csv['file_name'].append(filename.split('/')[-1])\n",
    "    #                 results_for_csv['class_id'].append(cls_idx+1)\n",
    "    #                 results_for_csv['confidence'].append(pos[4])\n",
    "    #                 results_for_csv['point1_x'].append(pos[0])\n",
    "    #                 results_for_csv['point1_y'].append(pos[1])\n",
    "    #                 results_for_csv['point2_x'].append(pos[2])\n",
    "    #                 results_for_csv['point2_y'].append(pos[1])\n",
    "    #                 results_for_csv['point3_x'].append(pos[2])\n",
    "    #                 results_for_csv['point3_y'].append(pos[3])\n",
    "    #                 results_for_csv['point4_x'].append(pos[0])\n",
    "    #                 results_for_csv['point4_y'].append(pos[3])\n",
    "            #confidence score가 0.5보다 클 때\n",
    "            # if result[np.where(result[:,4]>0.5)].shape[0]!=0:\n",
    "            #         print('11')\n",
    "    if t_idx%100==0:\n",
    "        print(str(t_idx) + ' end!')\n",
    "     \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5dd1699-276d-4180-b417-3b5ee1e0bbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(results_for_csv)\n",
    "submission.to_csv('HJS_VER1_WBF_TH_IOU0.55_CS_TH0.2_only_resnest.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ae988e-937f-4b0a-ad02-1ab9c49075df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
